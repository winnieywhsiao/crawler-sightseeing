{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "driver =  webdriver.Chrome(\"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chromedriver.exe\")\n",
    "addr = \"https://www.taiwan.net.tw/m1.aspx?sNo=0001123\"\n",
    "driver.get(addr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "tStart = time.time()\n",
    "\n",
    "id_n = []\n",
    "sites = []\n",
    "focus = []\n",
    "id_to = []\n",
    "id_name = []\n",
    "space = []\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "#id\n",
    "def get_id():\n",
    "    for id_c in soup.find_all('a','card-link'):\n",
    "        #print(id_c.get('href'))\n",
    "        n = id_c.get('href')\n",
    "        newid = n[23:]\n",
    "        #print(newid)\n",
    "        id_n.append(newid)\n",
    "        space.append(\" \")\n",
    "    return id_n, space\n",
    "        \n",
    "#地點名稱       \n",
    "def get_site():\n",
    "    for ele in soup.select('div.card-title'):\n",
    "        #print(ele.text)\n",
    "        sites.append(ele.text)\n",
    "    return sites\n",
    "      \n",
    "#點擊次數\n",
    "def get_focus():\n",
    "    for i in soup.select('span.target.target-like'):\n",
    "        #print(i.text)\n",
    "        focus.append(i.text)\n",
    "    return focus\n",
    "\n",
    "# id_to\n",
    "def get_toid():\n",
    "    count = 0\n",
    "\n",
    "    for q in soup.select('div.hashtag a'):\n",
    "        count += 1\n",
    "        if q.text not in id_name:\n",
    "            id_name.append(q.text)\n",
    "\n",
    "    for co in range(count):\n",
    "        href = soup.select('div.hashtag a')[co]['href']\n",
    "        #print(href)\n",
    "        newhref = href[23:]\n",
    "        if newhref not in id_to:\n",
    "            id_to.append(newhref)\n",
    "\n",
    "    return id_to, id_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag = []\n",
    "address = []\n",
    "opentime = []\n",
    "fro = []\n",
    "to = []\n",
    "\n",
    "def get_Info():\n",
    "    for j in range(1,len(sites)+1):\n",
    "        #print(len(sites)+1) # 1~42\n",
    "        num = str(j)\n",
    "\n",
    "        page = driver.find_element_by_xpath(\"/html/body/form/div[5]/div[3]/div[3]/ul/li[\" + num + \"]/div/div/a\")\n",
    "        #解決elenium中無法點擊Element：ElementClickInterceptedException\n",
    "        driver.execute_script(\"arguments[0].click();\", page)\n",
    "\n",
    "        n = str(id_n[j-1])\n",
    "        action = soup.find('form', id='form1').get('action')\n",
    "        re_action = action.replace('./','https://www.taiwan.net.tw/')\n",
    "        #取得各個景點網址\n",
    "        re_a = re_action + '&id=' + n\n",
    "\n",
    "        driver.get(re_a)\n",
    "        soup2 = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        #地址\n",
    "        for k in soup2.select('dd a.tel-link.address'):\n",
    "            address.append(k.text)\n",
    "            #print(k.text)\n",
    "\n",
    "        #開放時間\n",
    "        opt = []\n",
    "        if soup2.find('div','blk-zone') is not None:\n",
    "            if len(soup2.find('div','blk-zone'))>2:\n",
    "                for op in soup2.select('div.blk-zone p'):\n",
    "                    opt.append(op.text)\n",
    "                opentime.append(opt)               \n",
    "\n",
    "            else:\n",
    "                for op in soup2.select('div.blk-zone p'):\n",
    "                    opentime.append(op.text)\n",
    "                    #print(op.text)\n",
    "        else:\n",
    "            opentime.append(\"N/A\")\n",
    "            #print(\"NONE\")\n",
    "\n",
    "        #屬性\n",
    "        tag = []\n",
    "\n",
    "        for h in soup2.select('div.hashtag a'):\n",
    "            #print(h.text)\n",
    "            tag.append(h.text)\n",
    "        hashtag.append(tag)\n",
    "        \n",
    "        #from&to\n",
    "        count = 0\n",
    "\n",
    "        for q in soup2.select('div.hashtag a'):\n",
    "            count += 1\n",
    "\n",
    "        for co in range(count):\n",
    "            #id\n",
    "            id_c = soup2.find('form', id='form1').get('action')\n",
    "            newid = id_c[25:]\n",
    "            #print(newid)\n",
    "            fro.append(newid)\n",
    "\n",
    "            href = soup2.select('div.hashtag a')[co]['href']\n",
    "            #print(href)\n",
    "            newhref = href[23:]\n",
    "            to.append(newhref)\n",
    "\n",
    "        driver.back()\n",
    "        \n",
    "    return hashtag, address, opentime, fro, to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Time： 1100.1617755889893\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "i, sp = get_id()\n",
    "s = get_site()\n",
    "f = get_focus()\n",
    "id_to, id_name= get_toid()\n",
    "h, a, o, fro, to = get_Info()\n",
    "\n",
    "\n",
    "#輸出為csv檔\n",
    "# i+ to 可行\n",
    "df = pd.DataFrame({'id': i,\n",
    "                   'site_name': s,\n",
    "                   'opentime': o,\n",
    "                   'address': a,\n",
    "                   '關注度': f,\n",
    "                   '屬性': h,\n",
    "                   'type': sp})\n",
    "df.to_csv(\"site_Data.csv\", index=False, encoding='big5')\n",
    "# df.to_excel(\"site_ID.xlsx\", index=False, encoding='utf_8_sig')\n",
    "\n",
    "df2 = pd.DataFrame({'id': id_to,\n",
    "                    'type': id_name})\n",
    "df2.to_csv(\"site_attr.csv\", index=False, encoding='big5')\n",
    "\n",
    "df3 = pd.DataFrame({'from': fro,\n",
    "                    'to': to})\n",
    "df3.to_csv(\"site_Froto.csv\", index=False, encoding='big5')\n",
    "\n",
    "tEnd = time.time()\n",
    "print(\"Run Time：\",tEnd - tStart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = pd.read_csv(\"site_Data.csv\")\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to = []\n",
    "# to_name = []\n",
    "\n",
    "# def get_to():\n",
    "#     # from(id_n)\n",
    "#     # to\n",
    "#     count = 0\n",
    "\n",
    "#     for q in soup.select('div.hashtag a'):\n",
    "#         count += 1\n",
    "#         if q.text not in to_name:\n",
    "#             to_name.append(q.text)\n",
    "\n",
    "#     for co in range(count):\n",
    "#         href = soup.select('div.hashtag a')[co]['href']\n",
    "#         #print(href)\n",
    "#         newhref = href[23:]\n",
    "#         if newhref not in to:\n",
    "#             to.append(newhref)\n",
    "#     return to, to_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in range(len(ft)):\n",
    "#     print(ft[p])\n",
    "#     #ft[p]\n",
    "# print(len(ft))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
